{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Huize Huang - T06-03 [00] WIP Convolutional Neural Networks [Colab].ipynb","version":"0.3.2","provenance":[{"file_id":"19dVCXctEi1KW-SCr71kFDe5yIlY_xOzG","timestamp":1560970872343}],"private_outputs":true,"collapsed_sections":["axhMVhTdjZ-7"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"axhMVhTdjZ-7"},"source":["#### Copyright 2019 Google LLC."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bpiN4SUKjbOW","colab":{}},"source":["# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"A-ivEsaENhCB"},"source":["# WIP Convolutional Neural Networks"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XqPpkBh3Nnct"},"source":["Convolutional Neural Networks (CNN) are deep neural networks with the addition of two very special types of layers: convolutional layers and pooling layers. Let's take a look at both."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Agmd40ZSFOPN"},"source":["## Overview"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rY7emKinFRCr"},"source":["### Learning Objectives\n","\n","* Introduction to Convolutional Neural Networks\n","* Using TensorFlow to implement CNN"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"f3-v7hCmGI1K"},"source":["### Prerequisites\n","\n","* Introduction to TensorFLow\n","* Image Manipulation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"w-9f1tPWGNPG"},"source":["### Estimated Duration\n","\n","60 minutes"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eXOYSL4vGQ1r"},"source":["### Grading Criteria\n","\n","Each exercise is worth 3 points. The rubric for calculating those points is:\n","\n","| Points | Description |\n","|--------|-------------|\n","| 0      | No attempt at exercise |\n","| 1      | Attempted exercise, but code does not run |\n","| 2      | Attempted exercise, code runs, but produces incorrect answer |\n","| 3      | Exercise completed successfully |\n","\n","There is no graded exercise in this Colab so there is 0 points available."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YBKkcm6ROLLn"},"source":["## Convolutional Layers"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"537m9PSIONyR"},"source":["Convolutional layers are layers in the network that only partially connect to their input layers. The layer is divided into receptive fields that only look at a portion of the input layer and apply filters to it.\n","\n","Let's see this in action. First, we will create a 100 x 100 x 3 image that contains red vertical stripes centered every 10 pixels on the image."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PxJ9X6YaH6ta","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Create an image that is completely black\n","vertical_stripes = np.zeros((100, 100, 3))\n","\n","# Loop over the image 10 pixels at a time turning the\n","# center line of vertial pixels red\n","for x in range(4, 101, 10):\n","  vertical_stripes[:, x:x+2, 0] = 255\n","\n","plt.imshow(vertical_stripes)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OgPfyJyTLhPs","colab_type":"code","colab":{}},"source":["# Run this cell to mount your Google Drive.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"K0uXETOLP6Jc"},"source":["We will now create a filter that we'll apply using TensorFlow's [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) function. For illustrative purposes we'll create a filter to extract the red out of the image we just created.\n","\n","The filter will be 10 x 10 x 3 (remember that our vertical red lines are centered every 10 pixels and that our image has RGB values). The final number in the filter, 1, is the number of output channels we'd like the filter to produce. These output channels are called \"feature maps\". You get one feature map per filter."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kGs7yR7GLFz5","colab":{}},"source":["receptor_height, receptor_width, input_color_channels, output_color_channels = (10, 10, 3, 1)\n","filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels, output_color_channels), dtype=np.float32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-APXHrDtRNRy"},"source":["We created our filter and set it to all zeros. We now need to indicate what portion of the receptor field we want to extract data from. In this case we are trying to extract the vertical red line which we know is centered every ten pixels (pixels 5 and 6). To capture the red line we'll tell the filter that we only care about the 5th and 6th pixel in every row of data."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dL9L8RlsRMho","colab":{}},"source":["filters[:, 5:7, :, 0] = 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ze9FVE1VSBt_"},"source":["Let's now get our image ready to pass to our convolutional layer. To that we package the 3-dimensional image in yet another array to create a dataset for TensorFlow. TensorFlow's convolutional function expects a 4-dimensional dataset."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wxp8DjcxSKon","colab":{}},"source":["dataset = np.array([vertical_stripes], dtype=np.float32)\n","image_count, image_height, image_width, color_channels = dataset.shape\n","\n","image_count, image_height, image_width, color_channels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zS8Vmt-5TAgG"},"source":["To get the image into TensorFlow we need to create a placeholder. We'll create one that can take any number of images (None as the first value of `shape`). We could have also passed in `image_count` since we knew exactly how many images we were processing in this case, but it isn't required."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"y19a0mrfS-cy","colab":{}},"source":["import tensorflow as tf\n","\n","X = tf.placeholder(tf.float32, shape=(None, image_height, image_width, color_channels))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0rnO7icvTkYm"},"source":["To create our convolutional layer we use [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). The arguments we are passing it are:\n","\n","*  Our placeholder that we'll use to pass data into the layer.\n","*  The filters that we want to apply to the data. In this case we are passing in the filter that will capture the middle vertical pixels in a 10x10 receptor.\n","*  The strides we want the layer to take when operating on the data. In this case we want the input data to be processed for every image (the first 1) and every color channel (the last 1). The 10s cause the receptor to shift by 10 pixels every vertical and horizontal step through the image. This is exactly our filter size and allows us to stay centered on the red vertical lines. In practice you'd likely want some overlap.\n","*  A padding argument. In this case we chose \"SAME\" which causes TensorFlow to pad the image if necessary (equal padding on each size) in order to make the filter process the entire image."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JAVxgudyTjtX","colab":{}},"source":["convolution = tf.nn.conv2d(X, filters, strides=[1,10,10,1], padding=\"SAME\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sbdSrKvCUmob"},"source":["We can now run our convolutional layer using a TensorFlow session.\n","\n","Notice that our output shape reduces the input image to a 10 x 10 x 1 matrix from a 100 x 100 x 3 matrix. This is because we processed the image using a 10 x 10 single-channel-output filter and stepped 10 pixels each time. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JwRTfa2WLham","colab":{}},"source":["with tf.Session() as sess:\n","  output = sess.run(convolution, feed_dict={X: dataset})\n","\n","output.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pnS92oLHWK2u"},"source":["Looking at the image isn't very telling. It simply looks like a single-color image."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"a16GKcd9M4_i","colab":{}},"source":["plt.imshow(output[0, :, :, 0 ])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ynnmYDmDWTdQ"},"source":["If you look at the data you can see that the values are uniformly 2550."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q8Q-xZ0_WSad","colab":{}},"source":["np.unique(output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"anfmmmNtYAWQ"},"source":["What happens if we include some black pixels by increasing our vertical filter to capture the four vertical pixels in the center? Our output number changes to 5100.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1LEydsmQXepS","colab":{}},"source":["filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels, output_color_channels), dtype=np.float32)\n","filters[:, 4:8, :, :] = 1\n","\n","X = tf.placeholder(tf.float32, shape=(None, image_height, image_width, color_channels))\n","convolution = tf.nn.conv2d(X, filters, strides=[1,10,10,1], padding=\"SAME\")\n","\n","with tf.Session() as sess:\n","  output = sess.run(convolution, feed_dict={X: dataset})\n","\n","np.unique(output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Wqz32nZMYk7-"},"source":["If we move our filter to only capture black pixels our output becomes 0."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OH3g9ZFiXyDo","colab":{}},"source":["filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels, output_color_channels), dtype=np.float32)\n","filters[:, :2, :, :] = 1\n","\n","X = tf.placeholder(tf.float32, shape=(None, image_height, image_width, color_channels))\n","convolution = tf.nn.conv2d(X, filters, strides=[1,10,10,1], padding=\"SAME\")\n","\n","with tf.Session() as sess:\n","  output = sess.run(convolution, feed_dict={X: dataset})\n","\n","np.unique(output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eDDOOkvsZLRG"},"source":["Let's look at a convolutional layer on a real image. We'll load a sample image from Scikit Learn."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QoIJALbzZP48","colab":{}},"source":["from sklearn.datasets import load_sample_image\n","\n","china = load_sample_image('china.jpg')\n","\n","plt.imshow(china)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xsVJwfWcZWFz"},"source":["We will package the image in a 4-dimensional matrix for processing by TensorFlow."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zqzNI8cwZb4q","colab":{}},"source":["dataset = np.array([china], dtype=np.float32)\n","image_count, image_height, image_width, color_channels = dataset.shape\n","\n","image_count, image_height, image_width, color_channels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kdL-m32vZh5a"},"source":["Let's re-create our vertical line filter and apply it to the image to see the convolutional layer in action."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GiOxub1oY_Pb","colab":{}},"source":["receptor_height, receptor_width, input_color_channels, output_color_channels = (10, 10, 3, 1)\n","filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels, output_color_channels), dtype=np.float32)\n","filters[:, 5:7, :, :] = 1\n","\n","image_count, image_height, image_width, color_channels = dataset.shape\n","X = tf.placeholder(tf.float32, shape=(None, image_height, image_width, color_channels))\n","\n","convolution = tf.nn.conv2d(X, filters, strides=[1,4,4,1], padding=\"SAME\")\n","\n","with tf.Session() as sess:\n","  output = sess.run(convolution, feed_dict={X: dataset})\n","\n","plt.imshow(output[0, :, :, 0], cmap=\"gray\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Zh-uhsWqcfUD"},"source":["Typically you won't define your own filters though. You can let TensorFlow discover them by using [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d) instead of [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d).\n","\n","In this example we ask for three features with a 5x5 visual receptor stepping two pixels at a time."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ufZGmFneaTfE","colab":{}},"source":["image_count, image_height, image_width, color_channels = dataset.shape\n","X = tf.placeholder(tf.float32, shape=(None, image_height, image_width, color_channels))\n","\n","convolution = tf.layers.conv2d(X, filters=3, kernel_size=5, strides=[2,2], padding=\"SAME\")\n","\n","with tf.Session() as sess:\n","  sess.run(tf.global_variables_initializer())\n","  output = sess.run(convolution, feed_dict={X: dataset})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eVRW0OmFjJds"},"source":["Let's look at the first feature map."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AVy5abuBjIIZ","colab":{}},"source":["plt.imshow(output[0, :, :, 0])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NiKLRKzui1Qz"},"source":["Here is the second feature map."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HD7LNVBRiw4y","colab":{}},"source":["plt.imshow(output[0, :, :, 1])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LN0iB6JNi4Ox"},"source":["And the third."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WoLh4DO4iy74","colab":{}},"source":["plt.imshow(output[0, :, :, 2])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Wc3X_R2ac_g9"},"source":["## Pooling Layers"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ri1rpacidD3k"},"source":["Pooling layers are simply used to shrink the data from their input layer by sampling the data per receptor. Let's look at an example. We'll first load a sample image."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lJiUonz2dmnP","colab":{}},"source":["flower = load_sample_image('flower.jpg')\n","\n","plt.imshow(flower)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YsFrv0BCdrPX"},"source":["We can package this image in a 4-dimensional matrix and pass it to the [tf.nn.max_pool](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) function. This function extracts the maximum value from each receptor field.\n","\n","In the example below we create a 2 x 2 receptor (ksize) and move it around the image shifting 2 pixels each time. This reduces the height and width of the image by half, effectively reducing our dataset size by 75%."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"J_8YirYZswEH","colab":{}},"source":["dataset = np.array([flower], dtype=np.float32)\n","image_count, image_height, image_width, color_channels = dataset.shape\n","\n","X = tf.placeholder(tf.float32, shape=(None, image_height, image_width, color_channels))\n","max_pool = tf.nn.max_pool(X, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"VALID\")\n","\n","with tf.Session() as sess:\n","  output = sess.run(max_pool, feed_dict={X: dataset})\n","\n","plt.imshow(output[0].astype(np.uint8))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TChNDQJ2e0X4"},"source":["# Exercises"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yWwPocAWentg"},"source":["## Exercise 1: Challenge (Ungraded)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JD9H1pj9vK9z"},"source":["Use [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) to apply a stack of filters to the Scikit Learn built in flower image mentioned earlier in this colab.\n","\n","* Create a (7, 7, 3, 2) filter set. The `2` on the end indicates that we'll create two filters and get two output channels (feature maps).\n","* Make the first filter be a vertical line filter on the middle pixel of each row.\n","* Make the second filter be a horizontal line filter on the middle pixel of each row.\n","* Pass the flower image and filters to [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d), step 3 pixels vertically and horizontally.\n","* Display the first feature map as an image.\n","* Display the second feature map as an image."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QK1tz626LjZU"},"source":["### Student Solution"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PlBg4KCAgumu","colab":{}},"source":["# Create your filters and apply them to the flower image using TensorFlow here.\n","# receptor_height, receptor_width, input_color_channels, output_color_channels = (15, 15, 3, 2)\n","receptor_height, receptor_width, input_color_channels, output_color_channels = (7, 7, 3, 2)\n","filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels, output_color_channels), dtype=np.float32)\n","\n","dataset = np.array([flower], dtype=np.float32)\n","image_count, image_height, image_width, color_channels = dataset.shape\n","\n","# Use PyPlot to output the first feature map here.\n","filters[:, 4, :, 0] = 1\n","# filters[:, 7:10, :, 0] = 1\n","X = tf.placeholder(tf.float32, shape=(None, image_height, image_width, color_channels))\n","convolution = tf.nn.conv2d(X, filters, strides=[1,3,3,1], padding=\"SAME\")\n","\n","with tf.Session() as sess:\n","  output = sess.run(convolution, feed_dict={X: dataset})\n","\n","plt.imshow(output[0, :, :, 0], cmap=\"gray\")\n","# plt.imshow(output[0, :, :, 0])\n","plt.show()\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtZz2YyvQ4zg","colab_type":"code","colab":{}},"source":["# Use PyPlot to output the second feature map here.\n","filters[4, :, :, 1] = 1\n","X = tf.placeholder(tf.float32, shape=(None, image_height, image_width, color_channels))\n","convolution = tf.nn.conv2d(X, filters, strides=[1,3,3,1], padding=\"SAME\")\n","\n","with tf.Session() as sess:\n","  output = sess.run(convolution, feed_dict={X: dataset})\n","\n","plt.imshow(output[0, :, :, 0], cmap=\"gray\")\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_egYveeTlLL","colab_type":"code","colab":{}},"source":["image_count, image_height, image_width, color_channels = dataset.shape\n","X = tf.placeholder(tf.float32, shape=(None, image_height, image_width, color_channels))\n","\n","convolution = tf.layers.conv2d(X, filters=3, kernel_size=7, strides=[3,3], padding=\"SAME\")\n","\n","with tf.Session() as sess:\n","  sess.run(tf.global_variables_initializer())\n","  output = sess.run(convolution, feed_dict={X: dataset})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PJnXXZuETt1t","colab_type":"code","colab":{}},"source":["plt.imshow(output[0, :, :, 1])\n","plt.show()"],"execution_count":0,"outputs":[]}]}