{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KdK6a5-KL6GE"
   },
   "source": [
    "#### Copyright 2019 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tC70yXqNKJjw"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aX8dvKugMBst"
   },
   "source": [
    "# WIP Deep Neural Networks in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQJaGKZVMWhr"
   },
   "source": [
    "For this exercise we will train a deep neural network to interpret sign language from images of hands forming sign language shapes.\n",
    "\n",
    "The [dataset](https://www.kaggle.com/datamunge/sign-language-mnist) contains images of sign language symbols. The 'j' and 'z' are not included in the dataset since the require not just a hand form, but also a motion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bW7AVurPpb0u"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bd2Zkk1LE2Zr"
   },
   "source": [
    "### Learning Objectives\n",
    "\n",
    "  * Use TensorFlow to implement artificial neural network\n",
    "  * Use DNNClassifier to implement deep neural network classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCw0L_c7pg0L"
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "* Intermediate Python\n",
    "* Intermediate Pandas\n",
    "* Introduction to TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1GF_PMg4plQ-"
   },
   "source": [
    "### Estimated Duration\n",
    "\n",
    "60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4KJWVzfcpnBg"
   },
   "source": [
    "### Grading Criteria\n",
    "\n",
    "Each exercise is worth 3 points. The rubric for calculating those points is:\n",
    "\n",
    "| Points | Description |\n",
    "|--------|-------------|\n",
    "| 0      | No attempt at exercise |\n",
    "| 1      | Attempted exercise, but code does not run |\n",
    "| 2      | Attempted exercise, code runs, but produces incorrect answer |\n",
    "| 3      | Exercise completed successfully |\n",
    "\n",
    "There is no graded exercises in this Colab so there is 0 points available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y15pYY9p8uGD"
   },
   "source": [
    "## Acquire the Data\n",
    "\n",
    "Load the zipped sign language data into your Colab. The file should be named 'sign-language-mnist.zip'.\n",
    "\n",
    "After the file is uploaded, you can unzip the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lIX2s_XTMEn2"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('sign-language-mnist.zip', 'r')\n",
    "zip_ref.extractall('./')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gYCvlOIS84FJ"
   },
   "source": [
    "The unzipped data contains many different files. There is an overview image of American Sign Language (ASL), a few example images, and two CSV files. The dataset is already split into testing and training data.\n",
    "\n",
    "## Load and Examine the Training Data\n",
    "\n",
    "Let's load the data and describe it. As we can see there are 27,455 rows of data. Each row starts with a label signifying the letter in the image and 784 pixel that contain the data for a 28x28 image.\n",
    "\n",
    "The label contains values 0.0 through 24.0 and the pixels contain data between 0.0 and 255.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJAEVNF4Maij"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.318813</td>\n",
       "      <td>145.419377</td>\n",
       "      <td>148.500273</td>\n",
       "      <td>151.247714</td>\n",
       "      <td>153.546531</td>\n",
       "      <td>156.210891</td>\n",
       "      <td>158.411255</td>\n",
       "      <td>160.472154</td>\n",
       "      <td>162.339683</td>\n",
       "      <td>163.954799</td>\n",
       "      <td>...</td>\n",
       "      <td>141.104863</td>\n",
       "      <td>147.495611</td>\n",
       "      <td>153.325806</td>\n",
       "      <td>159.125332</td>\n",
       "      <td>161.969259</td>\n",
       "      <td>162.736696</td>\n",
       "      <td>162.906137</td>\n",
       "      <td>161.966454</td>\n",
       "      <td>161.137898</td>\n",
       "      <td>159.824731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.287552</td>\n",
       "      <td>41.358555</td>\n",
       "      <td>39.942152</td>\n",
       "      <td>39.056286</td>\n",
       "      <td>38.595247</td>\n",
       "      <td>37.111165</td>\n",
       "      <td>36.125579</td>\n",
       "      <td>35.016392</td>\n",
       "      <td>33.661998</td>\n",
       "      <td>32.651607</td>\n",
       "      <td>...</td>\n",
       "      <td>63.751194</td>\n",
       "      <td>65.512894</td>\n",
       "      <td>64.427412</td>\n",
       "      <td>63.708507</td>\n",
       "      <td>63.738316</td>\n",
       "      <td>63.444008</td>\n",
       "      <td>63.509210</td>\n",
       "      <td>63.298721</td>\n",
       "      <td>63.610415</td>\n",
       "      <td>64.396846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>125.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label        pixel1        pixel2        pixel3        pixel4  \\\n",
       "count  27455.000000  27455.000000  27455.000000  27455.000000  27455.000000   \n",
       "mean      12.318813    145.419377    148.500273    151.247714    153.546531   \n",
       "std        7.287552     41.358555     39.942152     39.056286     38.595247   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        6.000000    121.000000    126.000000    130.000000    133.000000   \n",
       "50%       13.000000    150.000000    153.000000    156.000000    158.000000   \n",
       "75%       19.000000    174.000000    176.000000    178.000000    179.000000   \n",
       "max       24.000000    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "             pixel5        pixel6        pixel7        pixel8        pixel9  \\\n",
       "count  27455.000000  27455.000000  27455.000000  27455.000000  27455.000000   \n",
       "mean     156.210891    158.411255    160.472154    162.339683    163.954799   \n",
       "std       37.111165     36.125579     35.016392     33.661998     32.651607   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      137.000000    140.000000    142.000000    144.000000    146.000000   \n",
       "50%      160.000000    162.000000    164.000000    165.000000    166.000000   \n",
       "75%      181.000000    182.000000    183.000000    184.000000    185.000000   \n",
       "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "       ...      pixel775      pixel776      pixel777      pixel778  \\\n",
       "count  ...  27455.000000  27455.000000  27455.000000  27455.000000   \n",
       "mean   ...    141.104863    147.495611    153.325806    159.125332   \n",
       "std    ...     63.751194     65.512894     64.427412     63.708507   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...     92.000000     96.000000    103.000000    112.000000   \n",
       "50%    ...    144.000000    162.000000    172.000000    180.000000   \n",
       "75%    ...    196.000000    202.000000    205.000000    207.000000   \n",
       "max    ...    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "           pixel779      pixel780      pixel781      pixel782      pixel783  \\\n",
       "count  27455.000000  27455.000000  27455.000000  27455.000000  27455.000000   \n",
       "mean     161.969259    162.736696    162.906137    161.966454    161.137898   \n",
       "std       63.738316     63.444008     63.509210     63.298721     63.610415   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      120.000000    125.000000    128.000000    128.000000    128.000000   \n",
       "50%      183.000000    184.000000    184.000000    182.000000    182.000000   \n",
       "75%      208.000000    207.000000    207.000000    206.000000    204.000000   \n",
       "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "           pixel784  \n",
       "count  27455.000000  \n",
       "mean     159.824731  \n",
       "std       64.396846  \n",
       "min        0.000000  \n",
       "25%      125.500000  \n",
       "50%      182.000000  \n",
       "75%      204.000000  \n",
       "max      255.000000  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./sign-language-mnist/sign_mnist_train.csv')\n",
    "\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "caoyLBoy-Ae3"
   },
   "source": [
    "Let's take a look at the distribution of the letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9PvhLL_C9xCg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0     1126\n",
       "1     1010\n",
       "2     1144\n",
       "3     1196\n",
       "4      957\n",
       "5     1204\n",
       "6     1090\n",
       "7     1013\n",
       "8     1162\n",
       "10    1114\n",
       "11    1241\n",
       "12    1055\n",
       "13    1151\n",
       "14    1196\n",
       "15    1088\n",
       "16    1279\n",
       "17    1294\n",
       "18    1199\n",
       "19    1186\n",
       "20    1161\n",
       "21    1082\n",
       "22    1225\n",
       "23    1164\n",
       "24    1118\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('label')['label'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98tD50QQ-HeX"
   },
   "source": [
    "We can see that letter 9, j, and letter 25, z, are missing as we expected. There seems to be a relatively even number of training samples for each letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gK0-35fj-avZ"
   },
   "source": [
    "Just to get an idea of the images that we are working with, let's take a look at a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pw2UgbAEXYts"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_to_view = 100\n",
    "\n",
    "plt.imshow(\n",
    "  train_df.iloc[image_to_view][train_df.columns.values[1:]].values.reshape(28,28),\n",
    "  cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viCOocFR-ZE-"
   },
   "source": [
    "## Load and Examine the Test Data\n",
    "\n",
    "Now we can load the test data and describe it. We can see that there are 7,172 test records, which is roughly a 20% sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqpafaX69WA-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "      <td>7172.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.247351</td>\n",
       "      <td>147.532627</td>\n",
       "      <td>150.445761</td>\n",
       "      <td>153.324317</td>\n",
       "      <td>155.663413</td>\n",
       "      <td>158.169688</td>\n",
       "      <td>160.790853</td>\n",
       "      <td>162.282766</td>\n",
       "      <td>163.649191</td>\n",
       "      <td>165.589515</td>\n",
       "      <td>...</td>\n",
       "      <td>138.546570</td>\n",
       "      <td>145.539598</td>\n",
       "      <td>150.744980</td>\n",
       "      <td>155.638873</td>\n",
       "      <td>158.893196</td>\n",
       "      <td>159.648494</td>\n",
       "      <td>158.162019</td>\n",
       "      <td>157.672755</td>\n",
       "      <td>156.664250</td>\n",
       "      <td>154.776771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.446712</td>\n",
       "      <td>43.593144</td>\n",
       "      <td>41.867838</td>\n",
       "      <td>40.442728</td>\n",
       "      <td>39.354776</td>\n",
       "      <td>37.749637</td>\n",
       "      <td>36.090916</td>\n",
       "      <td>36.212636</td>\n",
       "      <td>35.885378</td>\n",
       "      <td>33.721876</td>\n",
       "      <td>...</td>\n",
       "      <td>64.501665</td>\n",
       "      <td>65.132370</td>\n",
       "      <td>65.760539</td>\n",
       "      <td>65.565147</td>\n",
       "      <td>65.200300</td>\n",
       "      <td>65.499368</td>\n",
       "      <td>66.493576</td>\n",
       "      <td>66.009690</td>\n",
       "      <td>67.202939</td>\n",
       "      <td>68.285148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>113.750000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>106.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>179.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>204.250000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label       pixel1       pixel2       pixel3       pixel4  \\\n",
       "count  7172.000000  7172.000000  7172.000000  7172.000000  7172.000000   \n",
       "mean     11.247351   147.532627   150.445761   153.324317   155.663413   \n",
       "std       7.446712    43.593144    41.867838    40.442728    39.354776   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       4.000000   122.000000   126.000000   130.000000   134.000000   \n",
       "50%      11.000000   154.000000   157.000000   159.000000   161.000000   \n",
       "75%      18.000000   178.000000   179.000000   181.000000   182.000000   \n",
       "max      24.000000   255.000000   255.000000   255.000000   255.000000   \n",
       "\n",
       "            pixel5       pixel6       pixel7       pixel8       pixel9  ...  \\\n",
       "count  7172.000000  7172.000000  7172.000000  7172.000000  7172.000000  ...   \n",
       "mean    158.169688   160.790853   162.282766   163.649191   165.589515  ...   \n",
       "std      37.749637    36.090916    36.212636    35.885378    33.721876  ...   \n",
       "min       0.000000    10.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%     137.000000   141.000000   144.000000   145.000000   147.000000  ...   \n",
       "50%     163.000000   165.000000   166.000000   168.000000   169.000000  ...   \n",
       "75%     184.000000   185.000000   186.000000   187.000000   187.000000  ...   \n",
       "max     255.000000   255.000000   255.000000   255.000000   255.000000  ...   \n",
       "\n",
       "          pixel775     pixel776     pixel777     pixel778     pixel779  \\\n",
       "count  7172.000000  7172.000000  7172.000000  7172.000000  7172.000000   \n",
       "mean    138.546570   145.539598   150.744980   155.638873   158.893196   \n",
       "std      64.501665    65.132370    65.760539    65.565147    65.200300   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%      90.000000    95.000000    99.000000   105.000000   113.000000   \n",
       "50%     137.000000   155.000000   168.000000   177.000000   181.000000   \n",
       "75%     195.000000   200.000000   204.250000   207.000000   207.000000   \n",
       "max     255.000000   255.000000   255.000000   255.000000   255.000000   \n",
       "\n",
       "          pixel780     pixel781     pixel782     pixel783     pixel784  \n",
       "count  7172.000000  7172.000000  7172.000000  7172.000000  7172.000000  \n",
       "mean    159.648494   158.162019   157.672755   156.664250   154.776771  \n",
       "std      65.499368    66.493576    66.009690    67.202939    68.285148  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%     113.750000   113.000000   115.000000   111.000000   106.750000  \n",
       "50%     182.000000   181.000000   180.000000   180.000000   179.000000  \n",
       "75%     208.000000   207.000000   205.000000   206.000000   204.000000  \n",
       "max     255.000000   255.000000   255.000000   255.000000   255.000000  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./sign-language-mnist/sign_mnist_test.csv')\n",
    "\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMDRS_m5BS9X"
   },
   "source": [
    "Let's see the distribution of the labels in the test data. The test labels are also pretty easily distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CQHcuRxqM7rt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0     331\n",
       "1     432\n",
       "2     310\n",
       "3     245\n",
       "4     498\n",
       "5     247\n",
       "6     348\n",
       "7     436\n",
       "8     288\n",
       "10    331\n",
       "11    209\n",
       "12    394\n",
       "13    291\n",
       "14    246\n",
       "15    347\n",
       "16    164\n",
       "17    144\n",
       "18    246\n",
       "19    248\n",
       "20    266\n",
       "21    346\n",
       "22    206\n",
       "23    267\n",
       "24    332\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby('label')['label'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dx30ZwzuCNx-"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "We since our pixel values range from 0 through 255, their values might not work well in a neural network. It is best to scale down the data if possible. For this pixel data scaling is as easy as dividing by 255. We should also convert our label to an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dod09xZEWZ1Y"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.318813</td>\n",
       "      <td>0.570272</td>\n",
       "      <td>0.582354</td>\n",
       "      <td>0.593128</td>\n",
       "      <td>0.602143</td>\n",
       "      <td>0.612592</td>\n",
       "      <td>0.621221</td>\n",
       "      <td>0.629303</td>\n",
       "      <td>0.636626</td>\n",
       "      <td>0.642960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553352</td>\n",
       "      <td>0.578414</td>\n",
       "      <td>0.601278</td>\n",
       "      <td>0.624021</td>\n",
       "      <td>0.635174</td>\n",
       "      <td>0.638183</td>\n",
       "      <td>0.638848</td>\n",
       "      <td>0.635163</td>\n",
       "      <td>0.631913</td>\n",
       "      <td>0.626764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.287552</td>\n",
       "      <td>0.162190</td>\n",
       "      <td>0.156636</td>\n",
       "      <td>0.153162</td>\n",
       "      <td>0.151354</td>\n",
       "      <td>0.145534</td>\n",
       "      <td>0.141669</td>\n",
       "      <td>0.137319</td>\n",
       "      <td>0.132008</td>\n",
       "      <td>0.128046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250005</td>\n",
       "      <td>0.256913</td>\n",
       "      <td>0.252657</td>\n",
       "      <td>0.249837</td>\n",
       "      <td>0.249954</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>0.249056</td>\n",
       "      <td>0.248230</td>\n",
       "      <td>0.249453</td>\n",
       "      <td>0.252537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.474510</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.521569</td>\n",
       "      <td>0.537255</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.556863</td>\n",
       "      <td>0.564706</td>\n",
       "      <td>0.572549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.501961</td>\n",
       "      <td>0.501961</td>\n",
       "      <td>0.501961</td>\n",
       "      <td>0.492157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>0.619608</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.643137</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.650980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564706</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.674510</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.713725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.690196</td>\n",
       "      <td>0.698039</td>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768627</td>\n",
       "      <td>0.792157</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label        pixel1        pixel2        pixel3        pixel4  \\\n",
       "count  27455.000000  27455.000000  27455.000000  27455.000000  27455.000000   \n",
       "mean      12.318813      0.570272      0.582354      0.593128      0.602143   \n",
       "std        7.287552      0.162190      0.156636      0.153162      0.151354   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        6.000000      0.474510      0.494118      0.509804      0.521569   \n",
       "50%       13.000000      0.588235      0.600000      0.611765      0.619608   \n",
       "75%       19.000000      0.682353      0.690196      0.698039      0.701961   \n",
       "max       24.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "             pixel5        pixel6        pixel7        pixel8        pixel9  \\\n",
       "count  27455.000000  27455.000000  27455.000000  27455.000000  27455.000000   \n",
       "mean       0.612592      0.621221      0.629303      0.636626      0.642960   \n",
       "std        0.145534      0.141669      0.137319      0.132008      0.128046   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.537255      0.549020      0.556863      0.564706      0.572549   \n",
       "50%        0.627451      0.635294      0.643137      0.647059      0.650980   \n",
       "75%        0.709804      0.713725      0.717647      0.721569      0.725490   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...      pixel775      pixel776      pixel777      pixel778  \\\n",
       "count  ...  27455.000000  27455.000000  27455.000000  27455.000000   \n",
       "mean   ...      0.553352      0.578414      0.601278      0.624021   \n",
       "std    ...      0.250005      0.256913      0.252657      0.249837   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.360784      0.376471      0.403922      0.439216   \n",
       "50%    ...      0.564706      0.635294      0.674510      0.705882   \n",
       "75%    ...      0.768627      0.792157      0.803922      0.811765   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           pixel779      pixel780      pixel781      pixel782      pixel783  \\\n",
       "count  27455.000000  27455.000000  27455.000000  27455.000000  27455.000000   \n",
       "mean       0.635174      0.638183      0.638848      0.635163      0.631913   \n",
       "std        0.249954      0.248800      0.249056      0.248230      0.249453   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.470588      0.490196      0.501961      0.501961      0.501961   \n",
       "50%        0.717647      0.721569      0.721569      0.713725      0.713725   \n",
       "75%        0.815686      0.811765      0.811765      0.807843      0.800000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           pixel784  \n",
       "count  27455.000000  \n",
       "mean       0.626764  \n",
       "std        0.252537  \n",
       "min        0.000000  \n",
       "25%        0.492157  \n",
       "50%        0.713725  \n",
       "75%        0.800000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df.columns.values[0]] = train_df[train_df.columns.values[0]].astype(int)\n",
    "train_df[train_df.columns.values[1:]] = train_df[train_df.columns.values[1:]]/255.0\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6N9W3rSJCeoA"
   },
   "source": [
    "We need to scale and type the test data too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAUeQr9gWwkO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.584314</td>\n",
       "      <td>0.584314</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.592157</td>\n",
       "      <td>0.592157</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.592157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.580392</td>\n",
       "      <td>0.498039</td>\n",
       "      <td>0.349020</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.415686</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.419608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.501961</td>\n",
       "      <td>0.513725</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.521569</td>\n",
       "      <td>0.525490</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.407843</td>\n",
       "      <td>0.760784</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.560784</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.650980</td>\n",
       "      <td>0.949020</td>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.886275</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.878431</td>\n",
       "      <td>0.870588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.819608</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.819608</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.603922</td>\n",
       "      <td>0.972549</td>\n",
       "      <td>0.968627</td>\n",
       "      <td>0.972549</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.925490</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.737255</td>\n",
       "      <td>0.749020</td>\n",
       "      <td>0.756863</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.792157</td>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.796078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101961</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.250980</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.113725</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>0.207843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label    pixel1    pixel2    pixel3    pixel4    pixel5    pixel6  \\\n",
       "0      6  0.584314  0.584314  0.588235  0.588235  0.588235  0.592157   \n",
       "1      5  0.494118  0.501961  0.513725  0.517647  0.521569  0.525490   \n",
       "2     10  0.333333  0.345098  0.360784  0.376471  0.411765  0.482353   \n",
       "3      0  0.796078  0.803922  0.811765  0.807843  0.811765  0.819608   \n",
       "4      3  0.737255  0.749020  0.756863  0.764706  0.780392  0.788235   \n",
       "\n",
       "     pixel7    pixel8    pixel9  ...  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0  0.592157  0.588235  0.592157  ...  0.541176  0.580392  0.498039  0.349020   \n",
       "1  0.529412  0.529412  0.533333  ...  0.184314  0.407843  0.760784  0.717647   \n",
       "2  0.529412  0.560784  0.576471  ...  0.266667  0.650980  0.949020  0.890196   \n",
       "3  0.823529  0.819608  0.823529  ...  0.603922  0.972549  0.968627  0.972549   \n",
       "4  0.792157  0.796078  0.796078  ...  0.101961  0.156863  0.250980  0.188235   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0  0.321569  0.376471  0.415686  0.439216  0.470588  0.419608  \n",
       "1  0.729412  0.721569  0.721569  0.721569  0.713725  0.705882  \n",
       "2  0.901961  0.890196  0.886275  0.882353  0.878431  0.870588  \n",
       "3  0.992157  0.925490  0.901961  0.941176  0.992157  1.000000  \n",
       "4  0.113725  0.180392  0.192157  0.180392  0.180392  0.207843  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df.columns.values[0]] = test_df[test_df.columns.values[0]].astype(int)\n",
    "test_df[test_df.columns.values[1:]] = test_df[test_df.columns.values[1:]]/255.0\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxNjoIQNCt0N"
   },
   "source": [
    "## Build and Train the Model\n",
    "\n",
    "We now have some data ready to train and test our model. Let's begin by creating feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qW-fY1fXOFkL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.feature_column import numeric_column\n",
    "\n",
    "pixel_features = []\n",
    "\n",
    "for column_name in train_df.columns[1:]:\n",
    "  pixel_features.append(numeric_column(column_name))\n",
    "\n",
    "len(pixel_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AKPyZSv-GWvA"
   },
   "source": [
    "Creating a deep neural network classifier is as simple as creating an instance of a `DNNClassifier` class, declaring the number of classes, and choosing the number of hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTyqhoaqNKDY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0809 19:47:56.113234 4399982016 deprecation_wrapper.py:119] From /Users/dorishuang/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:10: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "W0809 19:47:56.118554 4399982016 estimator.py:1811] Using temporary folder as model directory: /var/folders/0n/ctf3gvvx57z27lg3_l0nbxzh0000gn/T/tmpip5rj4pb\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.estimator import DNNClassifier\n",
    "\n",
    "classifier = DNNClassifier(feature_columns=pixel_features, hidden_units=[392, 196], n_classes=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xR5gvu1KDTQw"
   },
   "source": [
    "We can now create an input function to feed the data to the model for training. The input function will create a dataset containing the feature and label columns. We'll shuffle the dataset, process it in batches, and repeat it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twNUGFQBQu2G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 19:47:56.182598 4399982016 deprecation.py:323] From /Users/dorishuang/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0809 19:47:57.483646 4399982016 deprecation.py:506] From /Users/dorishuang/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0809 19:48:05.732556 4399982016 deprecation.py:323] From /Users/dorishuang/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0809 19:48:05.875585 4399982016 deprecation.py:506] From /Users/dorishuang/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0809 19:48:12.970034 4399982016 deprecation.py:323] From /Users/dorishuang/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifier at 0x1201d8080>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "def training_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = train_df[column_name]\n",
    " \n",
    "  labels = train_df['label']\n",
    "\n",
    "  training_ds = Dataset.from_tensor_slices((features, labels))\n",
    "  training_ds = training_ds.shuffle(buffer_size=10000)\n",
    "  training_ds = training_ds.batch(100)\n",
    "  training_ds = training_ds.repeat(5)\n",
    "\n",
    "  return training_ds\n",
    "\n",
    "classifier.train(training_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Enk4WdmCDv63"
   },
   "source": [
    "## Test the Model\n",
    "\n",
    "We can now create an input function to feed our testing data into the model to make predictions. We'll use the model to make predictions and save them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0hJiUNn4fRS9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 19:51:36.899626 4399982016 deprecation.py:323] From /Users/dorishuang/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "def testing_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = test_df[column_name]\n",
    "  return Dataset.from_tensor_slices((features)).batch(1)\n",
    "\n",
    "predictions = list(classifier.predict(testing_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLROG-_GEDFC"
   },
   "source": [
    "We can now take a look at one of the predictions to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ag7dp0tKEBSm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': array([-5.3247657 , -0.22277446,  6.4307466 ,  1.3473305 , -1.4315459 ,\n",
       "         4.321315  ,  7.0797324 ,  5.812578  , -0.65925705, -8.193851  ,\n",
       "        -2.0875006 ,  4.2587843 , -1.1879572 ,  0.5829033 ,  5.2417927 ,\n",
       "         4.1937866 ,  5.2754326 , -3.0953314 , -1.3317766 ,  6.183665  ,\n",
       "        -0.42120564,  0.8082066 , -0.62047833,  4.3846464 ,  1.4311657 ,\n",
       "        -8.370553  ], dtype=float32),\n",
       " 'probabilities': array([1.4666829e-06, 2.4104762e-04, 1.8693487e-01, 1.1587528e-03,\n",
       "        7.1968119e-05, 2.2676485e-02, 3.5771841e-01, 1.0074493e-01,\n",
       "        1.5579048e-04, 8.3235363e-08, 3.7347610e-05, 2.1301936e-02,\n",
       "        9.1818300e-05, 5.3951592e-04, 5.6929085e-02, 1.9961398e-02,\n",
       "        5.8876749e-02, 1.3632236e-05, 7.9518744e-05, 1.4601049e-01,\n",
       "        1.9766297e-04, 6.7585293e-04, 1.6195052e-04, 2.4159074e-02,\n",
       "        1.2600853e-03, 6.9753789e-08], dtype=float32),\n",
       " 'class_ids': array([6]),\n",
       " 'classes': array([b'6'], dtype=object),\n",
       " 'all_class_ids': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25], dtype=int32),\n",
       " 'all_classes': array([b'0', b'1', b'2', b'3', b'4', b'5', b'6', b'7', b'8', b'9', b'10',\n",
       "        b'11', b'12', b'13', b'14', b'15', b'16', b'17', b'18', b'19',\n",
       "        b'20', b'21', b'22', b'23', b'24', b'25'], dtype=object)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7YTLR0uEv1j"
   },
   "source": [
    "We see the predicted class. We can also see the logits and probabilities for the class.\n",
    "\n",
    "Let's look at the actual label for our first bit of test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3r8AndxaFPmj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[0]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iz2ItpkAFVtl"
   },
   "source": [
    "We can also see if the image looks like the symbol we predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6PHrKWEFfOq"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_to_view = 0\n",
    "\n",
    "plt.imshow(\n",
    "  test_df.iloc[image_to_view][test_df.columns.values[1:]].values.reshape(28,28),\n",
    "  cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2dGW5wLGFrR1"
   },
   "source": [
    "We should now extract all of the class IDs from our predictions and calculate precision and recall.\n",
    "\n",
    "First, we'll extract all of the class IDs into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9hfI7ZmD_AF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 24,\n",
       " 10,\n",
       " 0,\n",
       " 20,\n",
       " 21,\n",
       " 20,\n",
       " 14,\n",
       " 20,\n",
       " 5,\n",
       " 20,\n",
       " 20,\n",
       " 17,\n",
       " 0,\n",
       " 19,\n",
       " 4,\n",
       " 10,\n",
       " 13,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 21,\n",
       " 13,\n",
       " 17,\n",
       " 15,\n",
       " 5,\n",
       " 15,\n",
       " 5,\n",
       " 10,\n",
       " 15,\n",
       " 13,\n",
       " 18,\n",
       " 19,\n",
       " 21,\n",
       " 17,\n",
       " 18,\n",
       " 21,\n",
       " 0,\n",
       " 14,\n",
       " 16,\n",
       " 17,\n",
       " 20,\n",
       " 15,\n",
       " 14,\n",
       " 24,\n",
       " 24,\n",
       " 0,\n",
       " 11,\n",
       " 18,\n",
       " 8,\n",
       " 20,\n",
       " 17,\n",
       " 19,\n",
       " 16,\n",
       " 16,\n",
       " 21,\n",
       " 0,\n",
       " 6,\n",
       " 20,\n",
       " 10,\n",
       " 18,\n",
       " 11,\n",
       " 20,\n",
       " 2,\n",
       " 20,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 21,\n",
       " 2,\n",
       " 16,\n",
       " 4,\n",
       " 20,\n",
       " 17,\n",
       " 15,\n",
       " 11,\n",
       " 2,\n",
       " 16,\n",
       " 17,\n",
       " 2,\n",
       " 16,\n",
       " 15,\n",
       " 5,\n",
       " 21,\n",
       " 0,\n",
       " 11,\n",
       " 8,\n",
       " 13,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 13,\n",
       " 21,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 21,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 20,\n",
       " 16,\n",
       " 1,\n",
       " 15,\n",
       " 14,\n",
       " 19,\n",
       " 2,\n",
       " 0,\n",
       " 21,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 11,\n",
       " 16,\n",
       " 5,\n",
       " 0,\n",
       " 20,\n",
       " 0,\n",
       " 13,\n",
       " 18,\n",
       " 13,\n",
       " 0,\n",
       " 20,\n",
       " 6,\n",
       " 15,\n",
       " 18,\n",
       " 15,\n",
       " 19,\n",
       " 5,\n",
       " 2,\n",
       " 10,\n",
       " 5,\n",
       " 24,\n",
       " 13,\n",
       " 15,\n",
       " 20,\n",
       " 21,\n",
       " 15,\n",
       " 13,\n",
       " 5,\n",
       " 24,\n",
       " 21,\n",
       " 11,\n",
       " 18,\n",
       " 11,\n",
       " 17,\n",
       " 21,\n",
       " 0,\n",
       " 11,\n",
       " 10,\n",
       " 11,\n",
       " 14,\n",
       " 18,\n",
       " 2,\n",
       " 10,\n",
       " 6,\n",
       " 24,\n",
       " 14,\n",
       " 11,\n",
       " 18,\n",
       " 18,\n",
       " 16,\n",
       " 20,\n",
       " 18,\n",
       " 21,\n",
       " 6,\n",
       " 20,\n",
       " 14,\n",
       " 16,\n",
       " 13,\n",
       " 15,\n",
       " 10,\n",
       " 10,\n",
       " 0,\n",
       " 5,\n",
       " 17,\n",
       " 1,\n",
       " 0,\n",
       " 16,\n",
       " 10,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 18,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 15,\n",
       " 13,\n",
       " 5,\n",
       " 19,\n",
       " 10,\n",
       " 5,\n",
       " 11,\n",
       " 1,\n",
       " 10,\n",
       " 20,\n",
       " 0,\n",
       " 1,\n",
       " 17,\n",
       " 17,\n",
       " 14,\n",
       " 17,\n",
       " 5,\n",
       " 14,\n",
       " 18,\n",
       " 13,\n",
       " 19,\n",
       " 20,\n",
       " 15,\n",
       " 13,\n",
       " 10,\n",
       " 0,\n",
       " 15,\n",
       " 5,\n",
       " 6,\n",
       " 10,\n",
       " 1,\n",
       " 2,\n",
       " 16,\n",
       " 11,\n",
       " 10,\n",
       " 13,\n",
       " 11,\n",
       " 19,\n",
       " 19,\n",
       " 24,\n",
       " 1,\n",
       " 17,\n",
       " 8,\n",
       " 20,\n",
       " 24,\n",
       " 24,\n",
       " 5,\n",
       " 24,\n",
       " 13,\n",
       " 10,\n",
       " 10,\n",
       " 16,\n",
       " 0,\n",
       " 10,\n",
       " 20,\n",
       " 16,\n",
       " 1,\n",
       " 20,\n",
       " 13,\n",
       " 10,\n",
       " 1,\n",
       " 20,\n",
       " 6,\n",
       " 13,\n",
       " 17,\n",
       " 11,\n",
       " 0,\n",
       " 10,\n",
       " 18,\n",
       " 16,\n",
       " 1,\n",
       " 20,\n",
       " 24,\n",
       " 14,\n",
       " 2,\n",
       " 10,\n",
       " 1,\n",
       " 21,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 10,\n",
       " 6,\n",
       " 0,\n",
       " 16,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 11,\n",
       " 5,\n",
       " 14,\n",
       " 0,\n",
       " 8,\n",
       " 21,\n",
       " 20,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 20,\n",
       " 5,\n",
       " 10,\n",
       " 13,\n",
       " 15,\n",
       " 8,\n",
       " 5,\n",
       " 18,\n",
       " 2,\n",
       " 18,\n",
       " 1,\n",
       " 0,\n",
       " 13,\n",
       " 18,\n",
       " 21,\n",
       " 0,\n",
       " 18,\n",
       " 0,\n",
       " 5,\n",
       " 8,\n",
       " 17,\n",
       " 16,\n",
       " 2,\n",
       " 17,\n",
       " 5,\n",
       " 17,\n",
       " 15,\n",
       " 13,\n",
       " 2,\n",
       " 5,\n",
       " 21,\n",
       " 11,\n",
       " 21,\n",
       " 14,\n",
       " 16,\n",
       " 1,\n",
       " 19,\n",
       " 20,\n",
       " 24,\n",
       " 5,\n",
       " 20,\n",
       " 1,\n",
       " 13,\n",
       " 20,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 0,\n",
       " 1,\n",
       " 24,\n",
       " 21,\n",
       " 20,\n",
       " 13,\n",
       " 15,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 24,\n",
       " 16,\n",
       " 18,\n",
       " 0,\n",
       " 10,\n",
       " 19,\n",
       " 10,\n",
       " 13,\n",
       " 11,\n",
       " 10,\n",
       " 20,\n",
       " 18,\n",
       " 20,\n",
       " 10,\n",
       " 6,\n",
       " 5,\n",
       " 24,\n",
       " 11,\n",
       " 17,\n",
       " 16,\n",
       " 15,\n",
       " 20,\n",
       " 1,\n",
       " 5,\n",
       " 24,\n",
       " 6,\n",
       " 15,\n",
       " 8,\n",
       " 20,\n",
       " 13,\n",
       " 16,\n",
       " 10,\n",
       " 0,\n",
       " 18,\n",
       " 0,\n",
       " 10,\n",
       " 17,\n",
       " 24,\n",
       " 17,\n",
       " 0,\n",
       " 10,\n",
       " 24,\n",
       " 14,\n",
       " 2,\n",
       " 18,\n",
       " 11,\n",
       " 11,\n",
       " 6,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 17,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 11,\n",
       " 14,\n",
       " 24,\n",
       " 5,\n",
       " 20,\n",
       " 0,\n",
       " 8,\n",
       " 11,\n",
       " 1,\n",
       " 10,\n",
       " 17,\n",
       " 2,\n",
       " 19,\n",
       " 1,\n",
       " 15,\n",
       " 18,\n",
       " 8,\n",
       " 20,\n",
       " 0,\n",
       " 5,\n",
       " 20,\n",
       " 14,\n",
       " 20,\n",
       " 24,\n",
       " 1,\n",
       " 19,\n",
       " 16,\n",
       " 17,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 15,\n",
       " 10,\n",
       " 6,\n",
       " 17,\n",
       " 15,\n",
       " 11,\n",
       " 18,\n",
       " 24,\n",
       " 10,\n",
       " 1,\n",
       " 13,\n",
       " 8,\n",
       " 17,\n",
       " 13,\n",
       " 24,\n",
       " 20,\n",
       " 0,\n",
       " 6,\n",
       " 21,\n",
       " 1,\n",
       " 19,\n",
       " 19,\n",
       " 21,\n",
       " 5,\n",
       " 13,\n",
       " 0,\n",
       " 0,\n",
       " 18,\n",
       " 6,\n",
       " 1,\n",
       " 20,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 16,\n",
       " 17,\n",
       " 10,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 18,\n",
       " 17,\n",
       " 5,\n",
       " 2,\n",
       " 19,\n",
       " 0,\n",
       " 15,\n",
       " 10,\n",
       " 11,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 13,\n",
       " 10,\n",
       " 11,\n",
       " 17,\n",
       " 20,\n",
       " 10,\n",
       " 5,\n",
       " 1,\n",
       " 18,\n",
       " 10,\n",
       " 21,\n",
       " 0,\n",
       " 18,\n",
       " 14,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 10,\n",
       " 18,\n",
       " 21,\n",
       " 20,\n",
       " 15,\n",
       " 19,\n",
       " 15,\n",
       " 18,\n",
       " 16,\n",
       " 8,\n",
       " 0,\n",
       " 13,\n",
       " 18,\n",
       " 0,\n",
       " 0,\n",
       " 17,\n",
       " 11,\n",
       " 20,\n",
       " 6,\n",
       " 10,\n",
       " 5,\n",
       " 8,\n",
       " 15,\n",
       " 18,\n",
       " 14,\n",
       " 19,\n",
       " 18,\n",
       " 13,\n",
       " 10,\n",
       " 21,\n",
       " 11,\n",
       " 11,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 15,\n",
       " 1,\n",
       " 11,\n",
       " 13,\n",
       " 18,\n",
       " 14,\n",
       " 2,\n",
       " 5,\n",
       " 21,\n",
       " 24,\n",
       " 6,\n",
       " 2,\n",
       " 18,\n",
       " 21,\n",
       " 10,\n",
       " 5,\n",
       " 19,\n",
       " 13,\n",
       " 0,\n",
       " 15,\n",
       " 1,\n",
       " 17,\n",
       " 0,\n",
       " 10,\n",
       " 16,\n",
       " 18,\n",
       " 10,\n",
       " 17,\n",
       " 0,\n",
       " 19,\n",
       " 21,\n",
       " 18,\n",
       " 13,\n",
       " 10,\n",
       " 1,\n",
       " 18,\n",
       " 2,\n",
       " 20,\n",
       " 10,\n",
       " 16,\n",
       " 8,\n",
       " 8,\n",
       " 15,\n",
       " 10,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 11,\n",
       " 2,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 20,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 24,\n",
       " 0,\n",
       " 2,\n",
       " 13,\n",
       " 0,\n",
       " 20,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 21,\n",
       " 16,\n",
       " 6,\n",
       " 15,\n",
       " 18,\n",
       " 14,\n",
       " 17,\n",
       " 15,\n",
       " 0,\n",
       " 11,\n",
       " 20,\n",
       " 0,\n",
       " 0,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 20,\n",
       " 10,\n",
       " 16,\n",
       " 15,\n",
       " 1,\n",
       " 15,\n",
       " 17,\n",
       " 6,\n",
       " 15,\n",
       " 5,\n",
       " 16,\n",
       " 17,\n",
       " 10,\n",
       " 0,\n",
       " 20,\n",
       " 13,\n",
       " 19,\n",
       " 13,\n",
       " 0,\n",
       " 21,\n",
       " 17,\n",
       " 11,\n",
       " 18,\n",
       " 14,\n",
       " 0,\n",
       " 20,\n",
       " 11,\n",
       " 11,\n",
       " 0,\n",
       " 21,\n",
       " 19,\n",
       " 13,\n",
       " 2,\n",
       " 13,\n",
       " 18,\n",
       " 5,\n",
       " 13,\n",
       " 5,\n",
       " 19,\n",
       " 5,\n",
       " 5,\n",
       " 13,\n",
       " 2,\n",
       " 8,\n",
       " 13,\n",
       " 24,\n",
       " 10,\n",
       " 5,\n",
       " 18,\n",
       " 8,\n",
       " 0,\n",
       " 10,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 18,\n",
       " 11,\n",
       " 15,\n",
       " 0,\n",
       " 10,\n",
       " 13,\n",
       " 1,\n",
       " 5,\n",
       " 18,\n",
       " 10,\n",
       " 21,\n",
       " 0,\n",
       " 24,\n",
       " 17,\n",
       " 1,\n",
       " 2,\n",
       " 20,\n",
       " 1,\n",
       " 0,\n",
       " 13,\n",
       " 11,\n",
       " 16,\n",
       " 15,\n",
       " 13,\n",
       " 15,\n",
       " 21,\n",
       " 21,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 18,\n",
       " 11,\n",
       " 10,\n",
       " 15,\n",
       " 16,\n",
       " 10,\n",
       " 16,\n",
       " 10,\n",
       " 11,\n",
       " 2,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 11,\n",
       " 11,\n",
       " 1,\n",
       " 15,\n",
       " 1,\n",
       " 0,\n",
       " 15,\n",
       " 5,\n",
       " 13,\n",
       " 0,\n",
       " 11,\n",
       " 24,\n",
       " 1,\n",
       " 11,\n",
       " 20,\n",
       " 1,\n",
       " 10,\n",
       " 10,\n",
       " 1,\n",
       " 19,\n",
       " 10,\n",
       " 20,\n",
       " 16,\n",
       " 11,\n",
       " 11,\n",
       " 2,\n",
       " 10,\n",
       " 20,\n",
       " 5,\n",
       " 19,\n",
       " 17,\n",
       " 0,\n",
       " 13,\n",
       " 20,\n",
       " 5,\n",
       " 11,\n",
       " 0,\n",
       " 18,\n",
       " 10,\n",
       " 5,\n",
       " 0,\n",
       " 24,\n",
       " 18,\n",
       " 1,\n",
       " 11,\n",
       " 8,\n",
       " 11,\n",
       " 20,\n",
       " 6,\n",
       " 10,\n",
       " 17,\n",
       " 16,\n",
       " 24,\n",
       " 1,\n",
       " 5,\n",
       " 17,\n",
       " 1,\n",
       " 21,\n",
       " 10,\n",
       " 6,\n",
       " 19,\n",
       " 16,\n",
       " 10,\n",
       " 0,\n",
       " 10,\n",
       " 21,\n",
       " 18,\n",
       " 10,\n",
       " 6,\n",
       " 0,\n",
       " 14,\n",
       " 10,\n",
       " 1,\n",
       " 17,\n",
       " 6,\n",
       " 16,\n",
       " 0,\n",
       " 17,\n",
       " 19,\n",
       " 6,\n",
       " 20,\n",
       " 13,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 10,\n",
       " 20,\n",
       " 18,\n",
       " 2,\n",
       " 18,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 19,\n",
       " 13,\n",
       " 17,\n",
       " 10,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 20,\n",
       " 11,\n",
       " 21,\n",
       " 24,\n",
       " 10,\n",
       " 20,\n",
       " 0,\n",
       " 20,\n",
       " 5,\n",
       " 1,\n",
       " 16,\n",
       " 15,\n",
       " 13,\n",
       " 15,\n",
       " 10,\n",
       " 10,\n",
       " 18,\n",
       " 15,\n",
       " 10,\n",
       " 20,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 19,\n",
       " 6,\n",
       " 17,\n",
       " 6,\n",
       " 2,\n",
       " 13,\n",
       " 16,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 18,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 19,\n",
       " 0,\n",
       " 17,\n",
       " 20,\n",
       " 8,\n",
       " 0,\n",
       " 21,\n",
       " 5,\n",
       " 19,\n",
       " 13,\n",
       " 5,\n",
       " 20,\n",
       " 13,\n",
       " 15,\n",
       " 10,\n",
       " 5,\n",
       " 17,\n",
       " 15,\n",
       " 10,\n",
       " 17,\n",
       " 1,\n",
       " 21,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 11,\n",
       " 1,\n",
       " 15,\n",
       " 2,\n",
       " 10,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 14,\n",
       " 16,\n",
       " 16,\n",
       " 2,\n",
       " 18,\n",
       " 10,\n",
       " 5,\n",
       " 18,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 24,\n",
       " 0,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 20,\n",
       " 13,\n",
       " 18,\n",
       " 5,\n",
       " 1,\n",
       " 13,\n",
       " 0,\n",
       " 11,\n",
       " 18,\n",
       " 24,\n",
       " 16,\n",
       " 0,\n",
       " 13,\n",
       " 18,\n",
       " 5,\n",
       " 18,\n",
       " 11,\n",
       " 20,\n",
       " 18,\n",
       " 18,\n",
       " 10,\n",
       " 2,\n",
       " 1,\n",
       " 19,\n",
       " 5,\n",
       " 8,\n",
       " 21,\n",
       " 8,\n",
       " 10,\n",
       " 17,\n",
       " 13,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 19,\n",
       " 17,\n",
       " 14,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 11,\n",
       " 0,\n",
       " 24,\n",
       " 17,\n",
       " 2,\n",
       " 13,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 21,\n",
       " 8,\n",
       " 10,\n",
       " 0,\n",
       " 21,\n",
       " 15,\n",
       " 5,\n",
       " 16,\n",
       " 0,\n",
       " 19,\n",
       " 20,\n",
       " 0,\n",
       " 13,\n",
       " 20,\n",
       " 5,\n",
       " 16,\n",
       " 10,\n",
       " 18,\n",
       " 16,\n",
       " 2,\n",
       " 10,\n",
       " 1,\n",
       " 5,\n",
       " 10,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes = [p['class_ids'][0] for p in predictions]\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V95MiB2pIXGk"
   },
   "source": [
    "We can now calculate the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UEtG03pNHkHQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29545454545454547"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(test_df['label'], predicted_classes, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wc_rUZ3WG5nU"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKPpxFZU7LqH"
   },
   "source": [
    "## Exercise 1: Challenge (Ungraded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9YNTDgIyTs2"
   },
   "source": [
    "The F1 score for this model is pretty poor. Check out the [documentation for the DNNClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) and see what parameters you can change.\n",
    "\n",
    "Experiment with different numbers and widths of hidden layers. Consider trying different optimizers and activation functions.\n",
    "\n",
    "Record the parameters that you set and your F1 score for those parameters below.\n",
    "\n",
    "Try to get an F1 above 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QK1tz626LjZU"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LThfFHrSI9cA"
   },
   "outputs": [],
   "source": [
    "### Run 1\n",
    "# Batch Size: 100\n",
    "# Data Set Repeats: 5\n",
    "# Layers: [392, 196]\n",
    "# Activation Function: relu\n",
    "# Loss Reduction: losses.Reduction.SUM\n",
    "# Optimizer: Adagrad\n",
    "# F1 Score: 0.12325711098717233\n",
    "\n",
    "from tensorflow.estimator import DNNClassifier\n",
    "from tensorflow import losses\n",
    "classifier = DNNClassifier(feature_columns=pixel_features, hidden_units=[392, 196],\n",
    "                           n_classes=26,activation_fn=tf.nn.relu,optimizer='Adagrad', loss_reduction=losses.Reduction.SUM,)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "def training_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = train_df[column_name]\n",
    " \n",
    "  labels = train_df['label']\n",
    "  training_ds = Dataset.from_tensor_slices((features, labels))\n",
    "  training_ds = training_ds.shuffle(buffer_size=10000)\n",
    "  training_ds = training_ds.batch(100)\n",
    "  training_ds = training_ds.repeat(5)\n",
    "  return training_ds\n",
    "\n",
    "classifier.train(training_input)\n",
    "\n",
    "def testing_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = test_df[column_name]\n",
    "  return Dataset.from_tensor_slices((features)).batch(1)\n",
    "\n",
    "predictions = list(classifier.predict(testing_input))\n",
    "\n",
    "predicted_classes = [p['class_ids'][0] for p in predictions]\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_df['label'], predicted_classes, average='micro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elVIDPIzn1Ak"
   },
   "outputs": [],
   "source": [
    "### Run 2\n",
    "# Batch Size: 200\n",
    "# Data Set Repeats: 10\n",
    "# Layers: [392,392,196]\n",
    "# Activation Function: sigmoid\n",
    "# Loss Reduction: SUM\n",
    "# Optimizer: GradientDescentOptimizer\n",
    "# F1 Score: 0.037228109313998885\n",
    "\n",
    "# ...\n",
    "import tensorflow as tf\n",
    "from tensorflow.estimator import DNNClassifier\n",
    "from tensorflow import losses\n",
    "\n",
    "classifier = DNNClassifier(feature_columns=pixel_features, hidden_units=[392, 392,196],\n",
    "                           n_classes=26,activation_fn=tf.nn.sigmoid,\n",
    "                           optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.3), loss_reduction=losses.Reduction.SUM,)\n",
    "\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "def training_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = train_df[column_name]\n",
    " \n",
    "  labels = train_df['label']\n",
    "  training_ds = Dataset.from_tensor_slices((features, labels))\n",
    "  training_ds = training_ds.shuffle(buffer_size=10000)\n",
    "  training_ds = training_ds.batch(200)\n",
    "  training_ds = training_ds.repeat(10)\n",
    "  return training_ds\n",
    "\n",
    "classifier.train(training_input)\n",
    "\n",
    "\n",
    "def testing_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = test_df[column_name]\n",
    "  return Dataset.from_tensor_slices((features)).batch(1)\n",
    "\n",
    "predictions = list(classifier.predict(testing_input))\n",
    "\n",
    "predicted_classes = [p['class_ids'][0] for p in predictions]\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_df['label'], predicted_classes, average='micro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9hf5KFbp3Cf"
   },
   "outputs": [],
   "source": [
    "### Run 3\n",
    "# Batch Size: 100\n",
    "# Data Set Repeats: 10\n",
    "# Layers: [196,52]\n",
    "# Activation Function: sigmoid\n",
    "# Loss Reduction: SUM\n",
    "# Optimizer: GradientDescentOptimizer\n",
    "# F1 Score: 0.15630228667038484\n",
    "\n",
    "# ...\n",
    "import tensorflow as tf\n",
    "from tensorflow.estimator import DNNClassifier\n",
    "from tensorflow import losses\n",
    "\n",
    "classifier = DNNClassifier(feature_columns=pixel_features, hidden_units=[196,52],\n",
    "                           n_classes=26,activation_fn=tf.nn.sigmoid,\n",
    "                           optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.03), loss_reduction=losses.Reduction.SUM_OVER_BATCH_SIZE,)\n",
    "\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "def training_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = train_df[column_name]\n",
    " \n",
    "  labels = train_df['label']\n",
    "  training_ds = Dataset.from_tensor_slices((features, labels))\n",
    "  training_ds = training_ds.shuffle(buffer_size=10000)\n",
    "  training_ds = training_ds.batch(100)\n",
    "  training_ds = training_ds.repeat(10)\n",
    "  return training_ds\n",
    "\n",
    "classifier.train(training_input)\n",
    "\n",
    "def testing_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = test_df[column_name]\n",
    "  return Dataset.from_tensor_slices((features)).batch(1)\n",
    "\n",
    "predictions = list(classifier.predict(testing_input))\n",
    "\n",
    "predicted_classes = [p['class_ids'][0] for p in predictions]\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_df['label'], predicted_classes, average='micro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XOUriq21sT3P"
   },
   "outputs": [],
   "source": [
    "### Run 4\n",
    "# Batch Size: 200\n",
    "# Data Set Repeats: 10\n",
    "# Layers: [392,392,196]\n",
    "# Activation Function: sigmoid\n",
    "# Loss Reduction: SUM\n",
    "# Optimizer: RMSPropOptimizer learning_rate=0.01\n",
    "# F1 Score: 0.48884551031790296\n",
    "\n",
    "# ...\n",
    "import tensorflow as tf\n",
    "from tensorflow.estimator import DNNClassifier\n",
    "from tensorflow import losses\n",
    "\n",
    "classifier = DNNClassifier(feature_columns=pixel_features, hidden_units=[392, 392],\n",
    "                           n_classes=26,activation_fn=tf.nn.sigmoid,\n",
    "                           optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01), loss_reduction=losses.Reduction.SUM,)\n",
    "\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "def training_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = train_df[column_name]\n",
    " \n",
    "  labels = train_df['label']\n",
    "  training_ds = Dataset.from_tensor_slices((features, labels))\n",
    "  training_ds = training_ds.shuffle(buffer_size=10000)\n",
    "  training_ds = training_ds.batch(200)\n",
    "  training_ds = training_ds.repeat(10)\n",
    "  return training_ds\n",
    "\n",
    "classifier.train(training_input)\n",
    "\n",
    "\n",
    "def testing_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = test_df[column_name]\n",
    "  return Dataset.from_tensor_slices((features)).batch(1)\n",
    "\n",
    "predictions = list(classifier.predict(testing_input))\n",
    "\n",
    "predicted_classes = [p['class_ids'][0] for p in predictions]\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_df['label'], predicted_classes, average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0JYsex37vR8n"
   },
   "outputs": [],
   "source": [
    "### Run 5\n",
    "# Batch Size: 250\n",
    "# Data Set Repeats: 10\n",
    "# Layers: [392,392]\n",
    "# Activation Function: sigmoid\n",
    "# Loss Reduction: SUM\n",
    "# Optimizer: RMSPropOptimizer learning_rate=0.001\n",
    "# F1 Score: 0.6429168990518683\n",
    "\n",
    "# ...\n",
    "import tensorflow as tf\n",
    "from tensorflow.estimator import DNNClassifier\n",
    "from tensorflow import losses\n",
    "\n",
    "classifier = DNNClassifier(feature_columns=pixel_features, hidden_units=[392, 392],\n",
    "                           n_classes=26,activation_fn=tf.nn.sigmoid,\n",
    "                           optimizer=tf.train.RMSPropOptimizer(learning_rate=0.001), loss_reduction=losses.Reduction.SUM,)\n",
    "\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "def training_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = train_df[column_name]\n",
    " \n",
    "  labels = train_df['label']\n",
    "  training_ds = Dataset.from_tensor_slices((features, labels))\n",
    "  training_ds = training_ds.shuffle(buffer_size=10000)\n",
    "  training_ds = training_ds.batch(250)\n",
    "  training_ds = training_ds.repeat(10)\n",
    "  return training_ds\n",
    "\n",
    "classifier.train(training_input)\n",
    "\n",
    "def testing_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = test_df[column_name]\n",
    "  return Dataset.from_tensor_slices((features)).batch(1)\n",
    "\n",
    "predictions = list(classifier.predict(testing_input))\n",
    "\n",
    "predicted_classes = [p['class_ids'][0] for p in predictions]\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_df['label'], predicted_classes, average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gaKu-txvGXit"
   },
   "outputs": [],
   "source": [
    "### Run 6\n",
    "# Batch Size: 250\n",
    "# Data Set Repeats: 10\n",
    "# Layers: [392,392]\n",
    "# Activation Function: sigmoid\n",
    "# Loss Reduction: SUM\n",
    "# Optimizer: tf.train.AdamOptimizer learning_rate=0.001\n",
    "# F1 Score: 0.6985499163413273\n",
    "\n",
    "# ...\n",
    "import tensorflow as tf\n",
    "from tensorflow.estimator import DNNClassifier\n",
    "from tensorflow import losses\n",
    "\n",
    "classifier = DNNClassifier(feature_columns=pixel_features, hidden_units=[392, 392],\n",
    "                           n_classes=26,activation_fn=tf.nn.sigmoid,\n",
    "                           optimizer=tf.train.AdamOptimizer(learning_rate=0.001), loss_reduction=losses.Reduction.SUM,)\n",
    "\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "def training_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = train_df[column_name]\n",
    " \n",
    "  labels = train_df['label']\n",
    "  training_ds = Dataset.from_tensor_slices((features, labels))\n",
    "  training_ds = training_ds.shuffle(buffer_size=10000)\n",
    "  training_ds = training_ds.batch(250)\n",
    "  training_ds = training_ds.repeat(10)\n",
    "  return training_ds\n",
    "\n",
    "classifier.train(training_input)\n",
    "\n",
    "def testing_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = test_df[column_name]\n",
    "  return Dataset.from_tensor_slices((features)).batch(1)\n",
    "\n",
    "predictions = list(classifier.predict(testing_input))\n",
    "\n",
    "predicted_classes = [p['class_ids'][0] for p in predictions]\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_df['label'], predicted_classes, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esrs_N03IBvD"
   },
   "outputs": [],
   "source": [
    "### Run 7\n",
    "# Batch Size: 250\n",
    "# Data Set Repeats: 10\n",
    "# Layers: [392,392]\n",
    "# Activation Function: leaky_relu\n",
    "# Loss Reduction: SUM\n",
    "# Optimizer: tf.train.AdamOptimizer learning_rate=0.001\n",
    "# F1 Score: 0.7849972113775795\n",
    "\n",
    "# ...\n",
    "import tensorflow as tf\n",
    "from tensorflow.estimator import DNNClassifier\n",
    "from tensorflow import losses\n",
    "\n",
    "classifier = DNNClassifier(feature_columns=pixel_features, hidden_units=[392, 392],\n",
    "                           n_classes=26,activation_fn=tf.nn.leaky_relu,\n",
    "                           optimizer=tf.train.AdamOptimizer(learning_rate=0.001), loss_reduction=losses.Reduction.SUM,)\n",
    "\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "def training_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = train_df[column_name]\n",
    " \n",
    "  labels = train_df['label']\n",
    "  training_ds = Dataset.from_tensor_slices((features, labels))\n",
    "  training_ds = training_ds.shuffle(buffer_size=10000)\n",
    "  training_ds = training_ds.batch(250)\n",
    "  training_ds = training_ds.repeat(10)\n",
    "  return training_ds\n",
    "\n",
    "classifier.train(training_input)\n",
    "\n",
    "def testing_input():\n",
    "  features = {}\n",
    "  for i in range(1, len(pixel_features)+1):\n",
    "    column_name = 'pixel'+str(i)\n",
    "    features[column_name] = test_df[column_name]\n",
    "  return Dataset.from_tensor_slices((features)).batch(1)\n",
    "\n",
    "predictions = list(classifier.predict(testing_input))\n",
    "\n",
    "predicted_classes = [p['class_ids'][0] for p in predictions]\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_df['label'], predicted_classes, average='micro')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KdK6a5-KL6GE"
   ],
   "name": "Huize Huang - T06-01 [00] WIP Deep Neural Networks in Tensorflow [Colab].ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1BZn7CQNNKzv1PlhnRcrZZb81Hp7K3i83",
     "timestamp": 1560970586059
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
