{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mW4Y4c6tvYvG"
   },
   "source": [
    "#### Copyright 2019 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24p97VuTvYVT"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVmV0M74xwm7"
   },
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "452jWTVV230i"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGXJElrBvyVT"
   },
   "source": [
    "### Learning Objectives\n",
    "\n",
    "* Understand how to generate text using a LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VZe4Xj9tyQRD"
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "* T08-07 Sequence Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbbAmmIhynVu"
   },
   "source": [
    "### Estimated Duration\n",
    "\n",
    "40 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7MHrGEyty2VI"
   },
   "source": [
    "### Grading Criteria\n",
    "\n",
    "Each exercise is worth 3 points. The rubric for calculating those points is:\n",
    "\n",
    "| Points | Description |\n",
    "|--------|-------------|\n",
    "| 0      | No attempt at exercise |\n",
    "| 1      | Attempted exercise, but code does not run |\n",
    "| 2      | Attempted exercise, code runs, but produces incorrect answer |\n",
    "| 3      | Exercise completed successfully |\n",
    "\n",
    "There is 1 exercise in this Colab so there are 3 points available. The grading scale will be 3 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LboFaJ47r6x-"
   },
   "source": [
    "## Setup and Data Gathering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqXw7S2BW-M0"
   },
   "source": [
    "Before starting, we recommend enabling GPU acceletation. Go to Runtime -> Change runtime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGWlsBFrRkRT"
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducible results\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1lbmeqeNxvDo"
   },
   "source": [
    "Before running the next block, go to [this link](https://www.gutenberg.org/ebooks/11) and download the plain text version of *Alice in Wonderland* to your computer. Then rename it 'alice.txt', and upload to the files in this colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1OqiVXDbvJin"
   },
   "outputs": [],
   "source": [
    "filename = \"alice.txt\"\n",
    "\n",
    "raw_text = open(filename).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xbwwfgt4C1Ee"
   },
   "source": [
    "The most important hyperparameter for the LSTM that we will train on this text is the length of our input sequences: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aowjnOF5CwqZ"
   },
   "outputs": [],
   "source": [
    "n_steps = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2edEgGVicErj"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dF77WHFz06_7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99            get\n",
      "100          very\n",
      "101         tired\n",
      "102            of\n",
      "103       sitting\n",
      "104            by\n",
      "105           her\n",
      "106        sister\n",
      "107            on\n",
      "108    the\\nbank,\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "raw_text=pd.Series(raw_text.split(' '))\n",
    "\n",
    "print(raw_text[99:99+n_steps])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "am1qBEoiwxID"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLgPT01Ry4Yj"
   },
   "source": [
    "Spacy is a useful package with includes tools for tokenization (converting word to numbers), and embeddings (converting word-numbers to vectors, thereby avoiding one-hot endcoding.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PoTkcr-tQ3TY"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(raw_text)\n",
    "\n",
    "#Encode words as integers:\n",
    "text_tokens = tokenizer.texts_to_sequences(raw_text)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RER5_lnUyMw1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 516, 5, 373, 52, 19, 592, 23, 1, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "x=[]\n",
    "for i in range(len(text_tokens)):\n",
    "  if len(text_tokens[i])>0:\n",
    "    x.append(text_tokens[i][0])\n",
    "    \n",
    "print(x[99:99+n_steps+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlgDLzhmxYeQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 27. 516.   5. 373.  52.  19. 592.  23.   1.   2.] 5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_samples = len(x)-n_steps\n",
    "\n",
    "X = np.zeros((n_samples, n_steps))\n",
    "y = np.zeros(n_samples)\n",
    "\n",
    "for i in range(len(x)-n_steps-1):\n",
    "    X[i] = x[i:i + n_steps] #lists of n_steps words\n",
    "    y[i] = x[i + n_steps] #next word in list\n",
    "\n",
    "print(X[99],y[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDNFvqlCxJU8"
   },
   "source": [
    "# Setting up and Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kfxoQKuSv2h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "#Create one-hot encoding of target, y:\n",
    "y_binary = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WbHfrarYSw3n"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0809 19:42:40.895305 4445066688 deprecation.py:506] From /Users/dorishuang/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0809 19:42:40.898790 4445066688 deprecation.py:506] From /Users/dorishuang/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0809 19:42:41.260365 4445066688 deprecation.py:323] From /Users/dorishuang/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 300)           1419900   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4733)              478033    \n",
      "=================================================================\n",
      "Total params: 2,058,333\n",
      "Trainable params: 2,058,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# setup model\n",
    "embedding_dim=300\n",
    "\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    trainable=True,\n",
    "    mask_zero=True, \n",
    "    input_length=n_steps\n",
    "  ),\n",
    "  keras.layers.LSTM(100, dropout=0.1),\n",
    "  keras.layers.Dense(vocab_size, activation='softmax')  \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zI_hGQG2DoAh"
   },
   "source": [
    "With GPU acceleration, training the model will take about 5 minutes. You can decrease this by reducing the number of epochs below, but this will lead to less accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3feJNa2S1z5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "26832/26832 [==============================] - 42s 2ms/sample - loss: 6.8255 - categorical_accuracy: 0.0607\n",
      "Epoch 2/40\n",
      "26832/26832 [==============================] - 42s 2ms/sample - loss: 6.3048 - categorical_accuracy: 0.0680\n",
      "Epoch 3/40\n",
      "26832/26832 [==============================] - 42s 2ms/sample - loss: 6.0785 - categorical_accuracy: 0.0845\n",
      "Epoch 4/40\n",
      "26832/26832 [==============================] - 45s 2ms/sample - loss: 5.8528 - categorical_accuracy: 0.1017\n",
      "Epoch 5/40\n",
      "26832/26832 [==============================] - 36s 1ms/sample - loss: 5.6180 - categorical_accuracy: 0.1185\n",
      "Epoch 6/40\n",
      "26832/26832 [==============================] - 40s 1ms/sample - loss: 5.3734 - categorical_accuracy: 0.1396\n",
      "Epoch 7/40\n",
      "26832/26832 [==============================] - 33s 1ms/sample - loss: 5.1374 - categorical_accuracy: 0.1605\n",
      "Epoch 8/40\n",
      " 7232/26832 [=======>......................] - ETA: 25s - loss: 4.8717 - categorical_accuracy: 0.1777"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-693d5bf3387d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_binary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mins\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    529\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    529\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['categorical_accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X,\n",
    "    y_binary,\n",
    "    epochs=40,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c90BtU7Zk3VH"
   },
   "source": [
    "#Using the model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HK6Xo_PoTC-s"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(input_text,n):\n",
    "  output_text=input_text\n",
    "  for i in range(n):\n",
    "    token_list = tokenizer.texts_to_sequences([output_text])[0][-n_steps:]\n",
    "    predicted = model.predict_classes(np.array(token_list)[np.newaxis,:])\n",
    "    next_word=tokenizer.index_word[predicted[0]]\n",
    "    output_text += \" \"+next_word\n",
    "    \n",
    "    if i%10==0:\n",
    "      output_text +='\\n'\n",
    "     \n",
    "  return output_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R7VeINTjDNOH"
   },
   "source": [
    "Feel free to change the \"seed\" text below, but make sure that there are at least n_steps words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xG-qL3iYTK7a"
   },
   "outputs": [],
   "source": [
    "print(generate_text('Alice fell down the rabbit hole and hit her head on the',100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2w_dTSM0z54f"
   },
   "source": [
    "Notice that the generated text is non-sensical, but is still somewhat Alice-in-Wonderland-esq. Also, basic grammatical rules seem to be roughly followed, in the sense that we don't see any strings of words like \"the the a the an\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdqWCg3eLldl"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xs-QlHuc6IbK"
   },
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axURTCgJwOjY"
   },
   "source": [
    "Train the model above on some other body of text you find on the internet. Some interesting results can be obtaind from song lyrics, Shakespeare, presidential tweets,  religous texts, and scientific papers. You'll almost certainly have to play with hyperparameters to get decent results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMw0t6QpAF8f"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Ptus2D5__5E"
   },
   "outputs": [],
   "source": [
    "# Your solution here.\n",
    "filename = \"Fur Farming.txt\"\n",
    "\n",
    "raw_text = open(filename).read()\n",
    "n_steps = 10\n",
    "raw_text=pd.Series(raw_text.split(' '))\n",
    "\n",
    "print(raw_text[99:99+n_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmViaObIG6A2"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(raw_text)\n",
    "\n",
    "#Encode words as integers:\n",
    "text_tokens = tokenizer.texts_to_sequences(raw_text)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "x=[]\n",
    "for i in range(len(text_tokens)):\n",
    "  if len(text_tokens[i])>0:\n",
    "    x.append(text_tokens[i][0])\n",
    "    \n",
    "print(x[99:99+n_steps+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1UqvE2RGG3vO"
   },
   "outputs": [],
   "source": [
    "n_samples = len(x)-n_steps\n",
    "\n",
    "X = np.zeros((n_samples, n_steps))\n",
    "y = np.zeros(n_samples)\n",
    "\n",
    "for i in range(len(x)-n_steps-1):\n",
    "    X[i] = x[i:i + n_steps] #lists of n_steps words\n",
    "    y[i] = x[i + n_steps] #next word in list\n",
    "\n",
    "print(X[99],y[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uSgZjLu4HL9O"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "#Create one-hot encoding of target, y:\n",
    "y_binary = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "# setup model\n",
    "embedding_dim=400\n",
    "\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    trainable=True,\n",
    "    mask_zero=True, \n",
    "    input_length=n_steps\n",
    "  ),\n",
    "  keras.layers.LSTM(200, dropout=0.1),\n",
    "  keras.layers.Dense(vocab_size, activation='softmax')  \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wd2YpSjWHTjT"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['categorical_accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X,\n",
    "    y_binary,\n",
    "    epochs=20,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RqhIWgwKHXDe"
   },
   "outputs": [],
   "source": [
    "def generate_text(input_text,n):\n",
    "  output_text=input_text\n",
    "  for i in range(n):\n",
    "    token_list = tokenizer.texts_to_sequences([output_text])[0][-n_steps:]\n",
    "    predicted = model.predict_classes(np.array(token_list)[np.newaxis,:])\n",
    "    next_word=tokenizer.index_word[predicted[0]]\n",
    "    output_text += \" \"+next_word\n",
    "    \n",
    "    if i%10==0:\n",
    "      output_text +='\\n'\n",
    "     \n",
    "  return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lW-O0Pp1HbXQ"
   },
   "outputs": [],
   "source": [
    "print(generate_text('Furs are essential to the entire commercialization and the recent price changes have impacted ',200))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mW4Y4c6tvYvG"
   ],
   "name": "Huize Huang - T08-08 [00] Text Generation [Colab]",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1pp5bmgUilH4k6s7_Mi9NUM_7oxXJkjUI",
     "timestamp": 1563307898937
    },
    {
     "file_id": "1wvZJenbXCeIqkCsvZnUlonQXHT0XTrwF",
     "timestamp": 1553803541992
    },
    {
     "file_id": "1TpS3e4-89BRHc3_URqIUQtFev_mXzMbw",
     "timestamp": 1544680749825
    },
    {
     "file_id": "1tKt2_k_n1-RBHb3qB91zUhp1sw7y5Z87",
     "timestamp": 1544572701805
    },
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_classification.ipynb",
     "timestamp": 1540403723198
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
